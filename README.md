## Hand Gesture Control for Solenoid Lock

A comprehensive hand gesture recognition system for controlling solenoid locks using computer vision. This project offers two distinct approaches: a simple rule-based gesture detection and an advanced machine learning approach with custom trained models.

### 🚀 Features

#### Simple Approach (gesture_control.py)
- **Open palm**: sends `COMMAND_OPEN` (default `b'1'`)
- **Fist**: sends `COMMAND_CLOSE` (default `b'0'`)
- **Auto-lock** after configurable timeout
- Real-time gesture detection using MediaPipe
- Lightweight, single-camera processing
- ESP32/Arduino serial communication

#### Machine Learning Approach (train.py + predict.py)
- **Custom gesture training** with neural network model
- **Dataset collection** tool for creating custom gestures
- **High accuracy** gesture recognition (90%+ confidence threshold)
- **Raspberry Pi support** with lgpio for direct GPIO control
- **Extensible** - easily add new gesture types

### 🛠️ Requirements

**Hardware:**
- Webcam (USB or built-in)
- ESP32/Arduino (for serial communication) OR Raspberry Pi (for direct GPIO)
- Solenoid lock with appropriate relay module
- 12V power supply for solenoid

**Software:**
- Python 3.11+ (Windows/Linux/Raspberry Pi)
- Dependencies listed in `requirements.txt`

Install Python packages:
```bash
python -m pip install --upgrade pip
pip install -r requirements.txt
```

### 📁 Project Structure

#### Core Files
- `gesture_control.py`: Simple rule-based gesture control (main app)
- `config/config.py`: Runtime configuration (serial, camera, thresholds)
- `requirements.txt`: Python dependencies

#### Machine Learning Files
- `dataset_take.py`: Dataset collection tool for training custom gestures
- `train.py`: Neural network training script
- `predict.py`: ML-based gesture recognition with GPIO control (Raspberry Pi)
- `dataset_landmarks.csv`: Collected gesture data (generated by dataset_take.py)
- `model_gestur.h5`: Trained neural network model (generated by train.py)
- `label_encoder.pkl`: Label encoder for gesture classes (generated by train.py)

#### Documentation
- `README.md`: This file
- `LICENSE`: MIT license

## 🎯 Quick Start

### Option 1: Simple Gesture Control (Recommended for beginners)

1. **Setup hardware**: Connect ESP32/Arduino to your computer via USB
2. **Configure**: Edit `config/config.py` with your COM port
3. **Run**: `python gesture_control.py`
4. **Control**: Open palm = unlock, fist = lock

### Option 2: Machine Learning Approach (Advanced users)

1. **Collect data**: `python dataset_take.py` (record your custom gestures)
2. **Train model**: `python train.py` (creates model_gestur.h5)
3. **Deploy**: `python predict.py` (Raspberry Pi with GPIO control)

## ⚙️ Configuration

### Basic Configuration (`config/config.py`)

#### Serial Communication
```python
ESP32_PORT = 'COM9'          # Your Arduino/ESP32 COM port
BAUD_RATE = 9600             # Serial communication speed
SERIAL_TIMEOUT = 1           # Serial timeout in seconds
SERIAL_STARTUP_DELAY = 2     # Delay for Arduino initialization
```

#### Camera Settings
```python
CAMERA_INDEX = 0             # Camera index (0 = default camera)
CAMERA_WIDTH = 640           # Camera resolution width
CAMERA_HEIGHT = 480          # Camera resolution height
WINDOW_TITLE = 'MediaPipe Hand Gesture Control'
```

#### Gesture Detection
```python
MIN_DETECTION_CONFIDENCE = 0.7  # Hand detection confidence threshold
MIN_TRACKING_CONFIDENCE = 0.5   # Hand tracking confidence threshold
AUTO_LOCK_SECONDS = 5           # Auto-lock timeout in seconds
COMMAND_OPEN = b'1'             # Command sent to unlock
COMMAND_CLOSE = b'0'            # Command sent to lock
```

#### Performance Optimization
```python
MAX_NUM_HANDS = 1               # Limit number of hands processed
MODEL_COMPLEXITY = 0            # 0=faster, 1/2=more accurate
FRAME_SCALE = 0.75              # Scale factor for processing (0.1-1.0)
PROCESS_EVERY_N_FRAMES = 1      # Process every N frames (1=every frame)
```

## 🚀 Detailed Usage Instructions

### Simple Gesture Control (gesture_control.py)

#### Windows PowerShell
```bash
# Optional: create a virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Configure your COM port in config/config.py
# Then run the application
python gesture_control.py
```

#### Linux/Raspberry Pi
```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run the application
python3 gesture_control.py
```

**Controls:**
- **Open palm** (index and middle fingertip above their PIP joints) → UNLOCK
- **Fist** (index fingertip below its PIP joint) → LOCK
- **Auto-lock** activates after `AUTO_LOCK_SECONDS` when no open palm detected
- Press `q` in the camera window to quit

### Machine Learning Approach

#### Step 1: Collect Training Data
```bash
python dataset_take.py
```
- Records gesture sequences for training
- Default gestures: `buka_kunci` (unlock) and `kunci` (lock)
- Creates `dataset_landmarks.csv` with hand landmark coordinates
- Press `S` to start recording each sequence
- Press `Q` to quit data collection

#### Step 2: Train the Model
```bash
python train.py
```
- Trains a neural network on collected data
- Creates `model_gestur.h5` (trained model)
- Creates `label_encoder.pkl` (gesture labels)
- Shows training accuracy and validation results
- Recommended: 80%+ accuracy for reliable performance

#### Step 3: Deploy with Raspberry Pi
```bash
python predict.py
```
- Uses trained model for real-time gesture recognition
- Requires Raspberry Pi with lgpio library
- Controls solenoid directly via GPIO pins
- 90%+ confidence threshold for gesture activation

## 🔧 Hardware Setup

### ESP32/Arduino Setup (Simple Approach)

#### Required Components
- ESP32 or Arduino board
- Relay module (5V)
- Solenoid lock (12V)
- 12V power supply
- Jumper wires

#### Arduino Code Example
```cpp
void setup() {
  Serial.begin(9600);
  pinMode(4, OUTPUT);  // Relay control pin
  digitalWrite(4, LOW); // Start locked
}

void loop() {
  if (Serial.available()) {
    char command = Serial.read();
    if (command == '1') {
      digitalWrite(4, HIGH);  // Unlock
    } else if (command == '0') {
      digitalWrite(4, LOW);   // Lock
    }
  }
}
```

### Raspberry Pi Setup (ML Approach)

#### Required Components
- Raspberry Pi (any model with GPIO)
- Relay module (3.3V/5V compatible)
- Solenoid lock (12V)
- 12V power supply
- Jumper wires

#### GPIO Configuration
- **GPIO 17** (BCM): Relay control signal
- **5V/3.3V**: Relay VCC
- **GND**: Common ground

#### Install lgpio (Raspberry Pi)
```bash
sudo apt update
sudo apt install python3-lgpio
# OR
pip install lgpio
```

### Wiring Diagrams

#### ESP32/Arduino Wiring
```
ESP32/Arduino          Relay Module          Solenoid Lock
┌─────────────┐       ┌─────────────┐       ┌─────────────┐
│    VIN      │──────▶│    VCC      │       │             │
│    GND      │──────▶│    GND      │       │             │
│   GPIO4     │──────▶│     IN      │       │             │
└─────────────┘       │             │       │             │
                      │    COM      │◀──────│ 12V Supply+ │
                      │     NO      │──────▶│ Solenoid+   │
                      └─────────────┘       │ Solenoid-   │◀──┐
                                           └─────────────┘   │
                                                            │
                                           ┌─────────────┐   │
                                           │ 12V Supply- │───┘
                                           └─────────────┘
```

#### Raspberry Pi Wiring
```
Raspberry Pi           Relay Module          Solenoid Lock
┌─────────────┐       ┌─────────────┐       ┌─────────────┐
│   5V/3.3V   │──────▶│    VCC      │       │             │
│    GND      │──────▶│    GND      │       │             │
│  GPIO17     │──────▶│     IN      │       │             │
└─────────────┘       │             │       │             │
                      │    COM      │◀──────│ 12V Supply+ │
                      │     NO      │──────▶│ Solenoid+   │
                      └─────────────┘       │ Solenoid-   │◀──┐
                                           └─────────────┘   │
                                                            │
                                           ┌─────────────┐   │
                                           │ 12V Supply- │───┘
                                           └─────────────┘
```

**Safety Notes:**
- Always disconnect power when wiring
- Double-check polarity on power connections
- Use appropriate gauge wire for 12V supply
- Ensure relay is rated for solenoid current draw

## 🔍 Troubleshooting

### Common Issues

#### Camera Problems
- **Camera not opening**: Adjust `CAMERA_INDEX` in `config/config.py` (try 0, 1, 2...)
- **Poor image quality**: Check camera resolution settings in config
- **Camera already in use**: Close other applications using the camera
- **No camera detected**: Verify camera drivers are installed

#### Serial Communication (ESP32/Arduino)
- **Serial port error**: Verify `ESP32_PORT` in config (Windows: Device Manager → Ports)
- **Permission denied**: Run as administrator or add user to dialout group (Linux)
- **Connection timeout**: Check baud rate matches Arduino code
- **Commands not working**: Verify Arduino is programmed and running

#### Performance Issues
- **High CPU usage**: 
  - Lower `MODEL_COMPLEXITY` to 0
  - Increase `PROCESS_EVERY_N_FRAMES` to 2 or higher
  - Reduce `FRAME_SCALE` to 0.5 or lower
  - Set `MAX_NUM_HANDS = 1`
- **Slow gesture detection**: Increase confidence thresholds
- **Window not rendering**: Ensure display is available and no other app locks camera

#### Machine Learning Issues
- **Low training accuracy**: Collect more diverse training data
- **Model not loading**: Ensure `model_gestur.h5` and `label_encoder.pkl` exist
- **Poor gesture recognition**: Retrain with more consistent gesture data
- **GPIO errors (Raspberry Pi)**: Install lgpio library and run with proper permissions

### Performance Optimization Tips

#### For Low-End Hardware
```python
# In config/config.py
MODEL_COMPLEXITY = 0        # Fastest processing
FRAME_SCALE = 0.5          # Half resolution
PROCESS_EVERY_N_FRAMES = 3 # Process every 3rd frame
MAX_NUM_HANDS = 1          # Single hand only
```

#### For High Accuracy
```python
# In config/config.py
MODEL_COMPLEXITY = 1           # Better accuracy
MIN_DETECTION_CONFIDENCE = 0.8 # Higher confidence
MIN_TRACKING_CONFIDENCE = 0.7  # Better tracking
FRAME_SCALE = 1.0             # Full resolution
```

## 📦 Dependencies

### Core Dependencies (requirements.txt)
```
tensorflow==2.16.1    # Neural network framework
mediapipe             # Hand landmark detection
opencv-python         # Computer vision library
numpy                 # Numerical computing
pandas                # Data manipulation
scikit-learn          # Machine learning utilities
```

### Platform-Specific Dependencies

#### Raspberry Pi Only
```bash
# For GPIO control
sudo apt install python3-lgpio
# OR
pip install lgpio
```

#### Windows Only
- **Visual C++ Redistributable** (for TensorFlow)
- **USB drivers** for ESP32/Arduino

### Optional Dependencies
```bash
# For development/debugging
pip install matplotlib seaborn  # Data visualization
pip install jupyter notebook    # Interactive development
```

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🙏 Acknowledgments

- **MediaPipe** team for excellent hand tracking
- **OpenCV** community for computer vision tools
- **TensorFlow** team for machine learning framework
- **Arduino/ESP32** community for microcontroller support
